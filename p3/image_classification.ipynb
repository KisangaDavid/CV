{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgO0k03dlZ-T"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this assignment, you will practice building and training Convolutional Neural Networks with Pytorch to solve computer vision tasks.  This assignment includes two sections, each involving different tasks:\n",
        "\n",
        "(1) Image Classification. Predict image-level category labels on two historically notable image datasets: **CIFAR-10** and **MNIST**.\n",
        "\n",
        "(2) Image Segmentation. Predict pixel-wise classification (semantic segmentation) on synthetic input images formed by superimposing MNIST images on top of CIFAR images.\n",
        "\n",
        "You will design your own models in each section and build the entire training/testing pipeline with PyTorch. \n",
        "PyTorch provides optimized implementations of the building blocks and additional utilities, both of which will be necessary for experiments on real datasets. It is highly recommended to read the official [documentation](https://pytorch.org/docs/stable/index.html) and [examples](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) before starting your implementation. There are some APIs that you'll find useful:\n",
        "[Layers](http://pytorch.org/docs/stable/nn.html),\n",
        "[Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity),\n",
        "[Loss functions](http://pytorch.org/docs/stable/nn.html#loss-functions),\n",
        "[Optimizers](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "It is highly recommended to use Google Colab and run the notebook on a GPU node. Check https://colab.research.google.com/ and look for tutorials online. To use a GPU go to Runtime -> Change runtime type and select GPU. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5lueGqqw6sR"
      },
      "source": [
        "# (1) Image Classification\n",
        "\n",
        "In this section, you will design and train an image classification network, which takes images as input and outputs vectors whose length equals the number of possible categories on **MNIST** and **CIFAR-10** datasets. \n",
        "\n",
        "You can design your models by borrowing ideas from recent architectures, e.g., ResNet, but you may not simply copy an entire existing model. \n",
        "\n",
        "For image classification, you can use a built-in dataset provided by [torchvision](https://pytorch.org/vision/stable/index.html), a PyTorch official extension for image tasks. \n",
        "\n",
        "To finish this section step by step, you need to:\n",
        "\n",
        "* Prepare data by building a dataset and dataloader. (with [torchvision](https://pytorch.org/vision/stable/index.html))\n",
        "\n",
        "* Implement training code (6 points) & testing code (6 points), including model saving and loading.\n",
        "\n",
        "* Construct a model (12 points) and choose an optimizer (3 points).\n",
        "\n",
        "* Describe what you did, any additional features you implemented, and/or any graphs you made in training and evaluating your network. Also report final test accuracy @100 epochs in a writeup: hw3.pdf (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2nAlwqqFzLfU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpFf3VOO1bT1"
      },
      "source": [
        "## Data Preparation:\n",
        "\n",
        "Setup a Dataset for training and testing.\n",
        "\n",
        "Datasets load single training examples one a time, so we practically wrap each Dataset in a DataLoader, which loads a data batch in parallel.\n",
        "\n",
        "We provide an example for setting up a training set for MNIST, and you should complete the rest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R_6rGLHwzMxp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Setting up MNIST data loaders\n",
        "mnist_train = torchvision.datasets.MNIST('./data', train = True, download = True, transform = T.ToTensor())\n",
        "mnist_train_data_loader = torch.utils.data.DataLoader(mnist_train,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\"./data\", train = False, download = True, transform = T.ToTensor())\n",
        "mnist_test_data_loader = torch.utils.data.DataLoader(mnist_test,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n",
        "# Setting up CIFAR10 data loaders\n",
        "cifar_train = torchvision.datasets.CIFAR10('./data', train = True, download = True, transform = T.ToTensor())\n",
        "cifar_train_data_loader = torch.utils.data.DataLoader(cifar_train,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n",
        "cifar_test = torchvision.datasets.CIFAR10(\"./data\", train = False, download = True, transform = T.ToTensor())\n",
        "cifar_test_data_loader = torch.utils.data.DataLoader(cifar_test,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58GUecr83BmH"
      },
      "source": [
        "## Design/choose your own model structure (12 points) and optimizer (3 points).\n",
        "You might want to adjust the following configurations for better performance:\n",
        "\n",
        "(1) Network architecture:\n",
        "- You can borrow some ideas from existing CNN designs, e.g., ResNet where\n",
        "the input from the previous layer is added to the output\n",
        "https://arxiv.org/abs/1512.03385\n",
        "- Note: Do not **directly copy** an entire existing network design.\n",
        "\n",
        "(2) Architecture hyperparameters:\n",
        "- Filter size, number of filters, and number of layers (depth). Make careful choices to tradeoff computational efficiency and accuracy.\n",
        "- Pooling vs. Strided Convolution\n",
        "- Batch normalization\n",
        "- Choice of non-linear activation\n",
        "\n",
        "(3) Choice of optimizer (e.g., SGD, Adam, Adagrad, RMSprop) and associated hyperparameters (e.g., learning rate, momentum)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4ZA5kw3X3qLy"
      },
      "outputs": [],
      "source": [
        "#Basic model, feel free to customize the layout to fit your model design.\n",
        "\n",
        "##########################################################################\n",
        "# TODO: YOUR CODE HERE\n",
        "# (1) Design the model for MNIST\n",
        "# (2) Design the model for CIFAR-10\n",
        "##########################################################################\n",
        "\n",
        "# model with 1 convolution layer \n",
        "class MnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistNet, self).__init__()\n",
        "        self.convLayers = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size=3, stride=1, padding=1), \n",
        "                                        nn.BatchNorm2d(8), \n",
        "                                        nn.ReLU(), \n",
        "                                        nn.MaxPool2d(2))\n",
        "        self.out = nn.Linear(14*14*8, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convLayers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.out(x)\n",
        "        return out\n",
        "\n",
        "class CifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CifarNet, self).__init__()\n",
        "        self.convLayers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size=3, stride=1, padding=1), \n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "\n",
        "\n",
        "            )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(8*8*64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convLayers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.out(x)\n",
        "        return out\n",
        "\n",
        "# Using Adam optimizer with suggested parameters from class, learning rate = 0.001, betas = 0.9, 0.999. Initialized in the last code block, shown below just for completeness\n",
        "### optimizer = optim.Adam(mnistNet1.parameters(), lr = 0.001, betas=(0.9,0.999))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOuB58GA5W_C"
      },
      "source": [
        "## Training (6 points)\n",
        "\n",
        "Train a model on the given dataset using the PyTorch Module API.\n",
        "\n",
        "Inputs:\n",
        "- loader_train: The loader from which train samples will be drawn from.\n",
        "- loader_test: The loader from which test samples will be drawn from.\n",
        "- model: A PyTorch Module giving the model to train.\n",
        "- optimizer: An Optimizer object we will use to train the model.\n",
        "- epochs: (Optional) A Python integer giving the number of epochs to train for.\n",
        "\n",
        "Returns: Nothing, but prints model accuracies during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0Kx_q5rv5ddg"
      },
      "outputs": [],
      "source": [
        "# Create a validation set using the training data loader (necessary because do not want to introduce another parameter \n",
        "# in the train() function)\n",
        "def validation_split_from_loader(loader_train):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for _, (x,y) in enumerate(loader_train):\n",
        "        features.append(x)\n",
        "        labels.append(y)\n",
        "    all_features = torch.cat(features)\n",
        "    all_labels = torch.cat(labels)\n",
        "    train_dataset = torch.utils.data.TensorDataset(all_features, all_labels)\n",
        "    train_set, val_set = random_split(train_dataset, [int(0.9 * len(train_dataset)), int(0.1 * len(train_dataset))])\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "    val_data_loader = torch.utils.data.DataLoader(val_set,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "    return train_data_loader, val_data_loader\n",
        "\n",
        "# Calculate accuracy on the validation set\n",
        "def validate(loader_validation, model, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    num_samples = 0\n",
        "    num_correct = 0\n",
        "    for t, (x,y) in enumerate(loader_validation):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # For validation using loss:\n",
        "        # loss = F.cross_entropy(model(x), y)\n",
        "        num_samples += x.size()[0]\n",
        "        outputs = torch.argmax(model(x),1)\n",
        "        num_correct += (outputs == y).sum()\n",
        "    return (num_correct / num_samples) * 100\n",
        "\n",
        "def train(loader_train, loader_test, model, optimizer, epochs=100, model_name=\"current_model\"):\n",
        "    # Working locally on laptop and desktop, sometimes no gpu available. Device is gpu if gpu is available, cpu otherwise\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    loader_train, loader_val = validation_split_from_loader(loader_train)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    prevValAcc = None\n",
        "    bestModelStateDict = None\n",
        "    num_times_val_decreased = 0\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if t % 100 == 0:\n",
        "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "       \n",
        "        valAcc = validate(loader_val, model, device)\n",
        "        print(f\"validation accuracy on validation set: {valAcc}\")\n",
        "        if prevValAcc == None:\n",
        "            prevValAcc = valAcc\n",
        "            bestModelStateDict = model.state_dict()\n",
        "            continue\n",
        "\n",
        "        # Early stopping condition\n",
        "        if valAcc < prevValAcc:\n",
        "             num_times_val_decreased += 1\n",
        "             if num_times_val_decreased > 3:\n",
        "                print(f\"EARLY STOP DUE TO DECREASED VALIDATION ACCURACY: USING MODEL AT EPOCH {e - num_times_val_decreased}\")\n",
        "                print(f\"best model validation accuracy: {prevValAcc}\")\n",
        "                break\n",
        "        else:\n",
        "            prevValAcc = valAcc\n",
        "            bestModelStateDict = model.state_dict()\n",
        "            num_times_val_decreased = 0\n",
        "    # saving model \n",
        "    torch.save(bestModelStateDict, model_name + '.pth')\n",
        "\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdxEMZpC6E9K"
      },
      "source": [
        "## Testing (6 points)\n",
        "Test a model using the PyTorch Module API.\n",
        "\n",
        "Inputs:\n",
        "- loader: The loader from which test samples will be drawn from.\n",
        "- model: A PyTorch Module giving the model to test.\n",
        "\n",
        "Returns: Nothing, but prints model accuracies during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KA5YBjxx6IeF"
      },
      "outputs": [],
      "source": [
        "def test(loader, model, model_name=\"current_model\"):\n",
        "    model.load_state_dict(torch.load(model_name + '.pth'))\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() # set model to evaluation mode\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            num_samples += x.size()[0]\n",
        "            outputs = torch.argmax(model(x),1)\n",
        "            num_correct += (outputs == y).sum()\n",
        "            \n",
        "    acc = num_correct / num_samples\n",
        "    print('Eval %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omwoZPA26V-a"
      },
      "source": [
        "Describe your design details in the writeup hw3.pdf. (3 points)\n",
        "\n",
        "Finish your model and optimizer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jM6GIIFP6XhG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Epoch 0, Iteration 0, loss = 2.3082\n",
            "Epoch 0, Iteration 100, loss = 1.4560\n",
            "Epoch 0, Iteration 200, loss = 1.4030\n",
            "Epoch 0, Iteration 300, loss = 1.5352\n",
            "Epoch 0, Iteration 400, loss = 1.2495\n",
            "Epoch 0, Iteration 500, loss = 0.7189\n",
            "Epoch 0, Iteration 600, loss = 0.9608\n",
            "Epoch 0, Iteration 700, loss = 1.0324\n",
            "Epoch 0, Iteration 800, loss = 1.1941\n",
            "validation accuracy on validation set: 64.22000122070312\n",
            "Epoch 1, Iteration 0, loss = 0.9160\n",
            "Epoch 1, Iteration 100, loss = 1.0343\n",
            "Epoch 1, Iteration 200, loss = 0.6910\n",
            "Epoch 1, Iteration 300, loss = 0.6907\n",
            "Epoch 1, Iteration 400, loss = 0.9765\n",
            "Epoch 1, Iteration 500, loss = 0.8560\n",
            "Epoch 1, Iteration 600, loss = 1.0971\n",
            "Epoch 1, Iteration 700, loss = 0.7546\n",
            "Epoch 1, Iteration 800, loss = 0.6864\n",
            "validation accuracy on validation set: 70.72000122070312\n",
            "Epoch 2, Iteration 0, loss = 0.8453\n",
            "Epoch 2, Iteration 100, loss = 0.5980\n",
            "Epoch 2, Iteration 200, loss = 0.7450\n",
            "Epoch 2, Iteration 300, loss = 0.4089\n",
            "Epoch 2, Iteration 400, loss = 0.8246\n",
            "Epoch 2, Iteration 500, loss = 0.5235\n",
            "Epoch 2, Iteration 600, loss = 0.5964\n",
            "Epoch 2, Iteration 700, loss = 0.6960\n",
            "Epoch 2, Iteration 800, loss = 0.6536\n",
            "validation accuracy on validation set: 74.53999328613281\n",
            "Epoch 3, Iteration 0, loss = 0.5574\n",
            "Epoch 3, Iteration 100, loss = 0.7569\n",
            "Epoch 3, Iteration 200, loss = 0.5887\n",
            "Epoch 3, Iteration 300, loss = 0.5216\n",
            "Epoch 3, Iteration 400, loss = 0.4071\n",
            "Epoch 3, Iteration 500, loss = 0.5009\n",
            "Epoch 3, Iteration 600, loss = 0.7810\n",
            "Epoch 3, Iteration 700, loss = 0.5963\n",
            "Epoch 3, Iteration 800, loss = 0.5964\n",
            "validation accuracy on validation set: 74.0999984741211\n",
            "Epoch 4, Iteration 0, loss = 0.3495\n",
            "Epoch 4, Iteration 100, loss = 0.4291\n",
            "Epoch 4, Iteration 200, loss = 0.3449\n",
            "Epoch 4, Iteration 300, loss = 0.4348\n",
            "Epoch 4, Iteration 400, loss = 0.4211\n",
            "Epoch 4, Iteration 500, loss = 0.5193\n",
            "Epoch 4, Iteration 600, loss = 0.3454\n",
            "Epoch 4, Iteration 700, loss = 0.6260\n",
            "Epoch 4, Iteration 800, loss = 0.4878\n",
            "validation accuracy on validation set: 76.58000183105469\n",
            "Epoch 5, Iteration 0, loss = 0.2993\n",
            "Epoch 5, Iteration 100, loss = 0.2974\n",
            "Epoch 5, Iteration 200, loss = 0.2413\n",
            "Epoch 5, Iteration 300, loss = 0.2442\n",
            "Epoch 5, Iteration 400, loss = 0.2869\n",
            "Epoch 5, Iteration 500, loss = 0.4654\n",
            "Epoch 5, Iteration 600, loss = 0.3501\n",
            "Epoch 5, Iteration 700, loss = 0.3675\n",
            "Epoch 5, Iteration 800, loss = 0.2452\n",
            "validation accuracy on validation set: 77.80000305175781\n",
            "Epoch 6, Iteration 0, loss = 0.2459\n",
            "Epoch 6, Iteration 100, loss = 0.2795\n",
            "Epoch 6, Iteration 200, loss = 0.1021\n",
            "Epoch 6, Iteration 300, loss = 0.2730\n",
            "Epoch 6, Iteration 400, loss = 0.2343\n",
            "Epoch 6, Iteration 500, loss = 0.2605\n",
            "Epoch 6, Iteration 600, loss = 0.3614\n",
            "Epoch 6, Iteration 700, loss = 0.1529\n",
            "Epoch 6, Iteration 800, loss = 0.2381\n",
            "validation accuracy on validation set: 76.94000244140625\n",
            "Epoch 7, Iteration 0, loss = 0.1052\n",
            "Epoch 7, Iteration 100, loss = 0.3218\n",
            "Epoch 7, Iteration 200, loss = 0.1858\n",
            "Epoch 7, Iteration 300, loss = 0.0939\n",
            "Epoch 7, Iteration 400, loss = 0.0637\n",
            "Epoch 7, Iteration 500, loss = 0.2319\n",
            "Epoch 7, Iteration 600, loss = 0.2906\n",
            "Epoch 7, Iteration 700, loss = 0.2193\n",
            "Epoch 7, Iteration 800, loss = 0.0957\n",
            "validation accuracy on validation set: 77.84000396728516\n",
            "Epoch 8, Iteration 0, loss = 0.0877\n",
            "Epoch 8, Iteration 100, loss = 0.0998\n",
            "Epoch 8, Iteration 200, loss = 0.1155\n",
            "Epoch 8, Iteration 300, loss = 0.2036\n",
            "Epoch 8, Iteration 400, loss = 0.0475\n",
            "Epoch 8, Iteration 500, loss = 0.2681\n",
            "Epoch 8, Iteration 600, loss = 0.1166\n",
            "Epoch 8, Iteration 700, loss = 0.0992\n",
            "Epoch 8, Iteration 800, loss = 0.1862\n",
            "validation accuracy on validation set: 76.95999908447266\n",
            "Epoch 9, Iteration 0, loss = 0.1470\n",
            "Epoch 9, Iteration 100, loss = 0.0597\n",
            "Epoch 9, Iteration 200, loss = 0.0973\n",
            "Epoch 9, Iteration 300, loss = 0.0430\n",
            "Epoch 9, Iteration 400, loss = 0.0762\n",
            "Epoch 9, Iteration 500, loss = 0.2544\n",
            "Epoch 9, Iteration 600, loss = 0.1320\n",
            "Epoch 9, Iteration 700, loss = 0.0137\n",
            "Epoch 9, Iteration 800, loss = 0.1113\n",
            "validation accuracy on validation set: 75.83999633789062\n",
            "Epoch 10, Iteration 0, loss = 0.0294\n",
            "Epoch 10, Iteration 100, loss = 0.0561\n",
            "Epoch 10, Iteration 200, loss = 0.0248\n",
            "Epoch 10, Iteration 300, loss = 0.0168\n",
            "Epoch 10, Iteration 400, loss = 0.2412\n",
            "Epoch 10, Iteration 500, loss = 0.0904\n",
            "Epoch 10, Iteration 600, loss = 0.0733\n",
            "Epoch 10, Iteration 700, loss = 0.0596\n",
            "Epoch 10, Iteration 800, loss = 0.1315\n",
            "validation accuracy on validation set: 77.19999694824219\n",
            "Epoch 11, Iteration 0, loss = 0.1886\n",
            "Epoch 11, Iteration 100, loss = 0.0075\n",
            "Epoch 11, Iteration 200, loss = 0.3089\n",
            "Epoch 11, Iteration 300, loss = 0.1611\n",
            "Epoch 11, Iteration 400, loss = 0.1466\n",
            "Epoch 11, Iteration 500, loss = 0.0824\n",
            "Epoch 11, Iteration 600, loss = 0.1621\n",
            "Epoch 11, Iteration 700, loss = 0.0233\n",
            "Epoch 11, Iteration 800, loss = 0.0488\n",
            "validation accuracy on validation set: 77.94000244140625\n",
            "Epoch 12, Iteration 0, loss = 0.1392\n",
            "Epoch 12, Iteration 100, loss = 0.0071\n",
            "Epoch 12, Iteration 200, loss = 0.0390\n",
            "Epoch 12, Iteration 300, loss = 0.0915\n",
            "Epoch 12, Iteration 400, loss = 0.0826\n",
            "Epoch 12, Iteration 500, loss = 0.1553\n",
            "Epoch 12, Iteration 600, loss = 0.0325\n",
            "Epoch 12, Iteration 700, loss = 0.3185\n",
            "Epoch 12, Iteration 800, loss = 0.1296\n",
            "validation accuracy on validation set: 77.72000122070312\n",
            "Epoch 13, Iteration 0, loss = 0.1364\n",
            "Epoch 13, Iteration 100, loss = 0.0694\n",
            "Epoch 13, Iteration 200, loss = 0.0097\n",
            "Epoch 13, Iteration 300, loss = 0.0311\n",
            "Epoch 13, Iteration 400, loss = 0.0167\n",
            "Epoch 13, Iteration 500, loss = 0.1579\n",
            "Epoch 13, Iteration 600, loss = 0.0301\n",
            "Epoch 13, Iteration 700, loss = 0.1976\n",
            "Epoch 13, Iteration 800, loss = 0.0984\n",
            "validation accuracy on validation set: 77.4000015258789\n",
            "Epoch 14, Iteration 0, loss = 0.0343\n",
            "Epoch 14, Iteration 100, loss = 0.0242\n",
            "Epoch 14, Iteration 200, loss = 0.2066\n",
            "Epoch 14, Iteration 300, loss = 0.0222\n",
            "Epoch 14, Iteration 400, loss = 0.0223\n",
            "Epoch 14, Iteration 500, loss = 0.0665\n",
            "Epoch 14, Iteration 600, loss = 0.0234\n",
            "Epoch 14, Iteration 700, loss = 0.0120\n",
            "Epoch 14, Iteration 800, loss = 0.0414\n",
            "validation accuracy on validation set: 78.12000274658203\n",
            "Epoch 15, Iteration 0, loss = 0.0119\n",
            "Epoch 15, Iteration 100, loss = 0.0445\n",
            "Epoch 15, Iteration 200, loss = 0.0784\n",
            "Epoch 15, Iteration 300, loss = 0.0082\n",
            "Epoch 15, Iteration 400, loss = 0.0183\n",
            "Epoch 15, Iteration 500, loss = 0.1027\n",
            "Epoch 15, Iteration 600, loss = 0.0409\n",
            "Epoch 15, Iteration 700, loss = 0.0253\n",
            "Epoch 15, Iteration 800, loss = 0.0860\n",
            "validation accuracy on validation set: 79.25999450683594\n",
            "Epoch 16, Iteration 0, loss = 0.0453\n",
            "Epoch 16, Iteration 100, loss = 0.0285\n",
            "Epoch 16, Iteration 200, loss = 0.0065\n",
            "Epoch 16, Iteration 300, loss = 0.0331\n",
            "Epoch 16, Iteration 400, loss = 0.0476\n",
            "Epoch 16, Iteration 500, loss = 0.2559\n",
            "Epoch 16, Iteration 600, loss = 0.0791\n",
            "Epoch 16, Iteration 700, loss = 0.0955\n",
            "Epoch 16, Iteration 800, loss = 0.0657\n",
            "validation accuracy on validation set: 78.37999725341797\n",
            "Epoch 17, Iteration 0, loss = 0.0026\n",
            "Epoch 17, Iteration 100, loss = 0.0048\n",
            "Epoch 17, Iteration 200, loss = 0.0500\n",
            "Epoch 17, Iteration 300, loss = 0.0445\n",
            "Epoch 17, Iteration 400, loss = 0.0780\n",
            "Epoch 17, Iteration 500, loss = 0.0270\n",
            "Epoch 17, Iteration 600, loss = 0.0298\n",
            "Epoch 17, Iteration 700, loss = 0.0214\n",
            "Epoch 17, Iteration 800, loss = 0.0411\n",
            "validation accuracy on validation set: 79.0\n",
            "Epoch 18, Iteration 0, loss = 0.0535\n",
            "Epoch 18, Iteration 100, loss = 0.0038\n",
            "Epoch 18, Iteration 200, loss = 0.0040\n",
            "Epoch 18, Iteration 300, loss = 0.0118\n",
            "Epoch 18, Iteration 400, loss = 0.0134\n",
            "Epoch 18, Iteration 500, loss = 0.0064\n",
            "Epoch 18, Iteration 600, loss = 0.0091\n",
            "Epoch 18, Iteration 700, loss = 0.0360\n",
            "Epoch 18, Iteration 800, loss = 0.0134\n",
            "validation accuracy on validation set: 78.31999969482422\n",
            "Epoch 19, Iteration 0, loss = 0.0039\n",
            "Epoch 19, Iteration 100, loss = 0.0060\n",
            "Epoch 19, Iteration 200, loss = 0.0126\n",
            "Epoch 19, Iteration 300, loss = 0.0233\n",
            "Epoch 19, Iteration 400, loss = 0.0422\n",
            "Epoch 19, Iteration 500, loss = 0.1089\n",
            "Epoch 19, Iteration 600, loss = 0.0358\n",
            "Epoch 19, Iteration 700, loss = 0.1108\n",
            "Epoch 19, Iteration 800, loss = 0.0111\n",
            "validation accuracy on validation set: 77.23999786376953\n",
            "EARLY STOP DUE TO DECREASED VALIDATION ACCURACY: USING MODEL AT EPOCH 15\n",
            "best model validation accuracy: 79.25999450683594\n"
          ]
        }
      ],
      "source": [
        "cifarNet1 = CifarNet()\n",
        "optimizer = optim.Adam(cifarNet1.parameters(), lr = 0.0005, betas=(0.9,0.999))\n",
        "\n",
        "# Train model on training data and save parameters as cifar_model.pth\n",
        "# Early stopping at around epoch 7, roughly 73.8% accuracy\n",
        "train(cifar_train_data_loader, cifar_test_data_loader, cifarNet1, optimizer, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval 7786 / 10000 correct (77.86)\n"
          ]
        }
      ],
      "source": [
        "# Load model and test it on the testing set\n",
        "test(cifar_test_data_loader, cifarNet1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wxvtO5oQGctE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Epoch 0, Iteration 0, loss = 2.1835\n",
            "Epoch 0, Iteration 100, loss = 0.4409\n",
            "Epoch 0, Iteration 200, loss = 0.2257\n",
            "Epoch 0, Iteration 300, loss = 0.4354\n",
            "Epoch 0, Iteration 400, loss = 0.3453\n",
            "Epoch 0, Iteration 500, loss = 0.2040\n",
            "Epoch 0, Iteration 600, loss = 0.1599\n",
            "Epoch 0, Iteration 700, loss = 0.2159\n",
            "Epoch 0, Iteration 800, loss = 0.1399\n",
            "Epoch 0, Iteration 900, loss = 0.0414\n",
            "Epoch 0, Iteration 1000, loss = 0.0594\n",
            "validation accuracy on validation set: 96.16666412353516\n",
            "Epoch 1, Iteration 0, loss = 0.1561\n",
            "Epoch 1, Iteration 100, loss = 0.1090\n",
            "Epoch 1, Iteration 200, loss = 0.1599\n",
            "Epoch 1, Iteration 300, loss = 0.2414\n",
            "Epoch 1, Iteration 400, loss = 0.0914\n",
            "Epoch 1, Iteration 500, loss = 0.1241\n",
            "Epoch 1, Iteration 600, loss = 0.0424\n",
            "Epoch 1, Iteration 700, loss = 0.0278\n",
            "Epoch 1, Iteration 800, loss = 0.2441\n",
            "Epoch 1, Iteration 900, loss = 0.0592\n",
            "Epoch 1, Iteration 1000, loss = 0.0688\n",
            "validation accuracy on validation set: 97.06666564941406\n",
            "Epoch 2, Iteration 0, loss = 0.0139\n",
            "Epoch 2, Iteration 100, loss = 0.0457\n",
            "Epoch 2, Iteration 200, loss = 0.0661\n",
            "Epoch 2, Iteration 300, loss = 0.1613\n",
            "Epoch 2, Iteration 400, loss = 0.1038\n",
            "Epoch 2, Iteration 500, loss = 0.3617\n",
            "Epoch 2, Iteration 600, loss = 0.0562\n",
            "Epoch 2, Iteration 700, loss = 0.0305\n",
            "Epoch 2, Iteration 800, loss = 0.0291\n",
            "Epoch 2, Iteration 900, loss = 0.0741\n",
            "Epoch 2, Iteration 1000, loss = 0.0927\n",
            "validation accuracy on validation set: 97.31666564941406\n",
            "Epoch 3, Iteration 0, loss = 0.0539\n",
            "Epoch 3, Iteration 100, loss = 0.0495\n",
            "Epoch 3, Iteration 200, loss = 0.0504\n",
            "Epoch 3, Iteration 300, loss = 0.0241\n",
            "Epoch 3, Iteration 400, loss = 0.0347\n",
            "Epoch 3, Iteration 500, loss = 0.0717\n",
            "Epoch 3, Iteration 600, loss = 0.0985\n",
            "Epoch 3, Iteration 700, loss = 0.0359\n",
            "Epoch 3, Iteration 800, loss = 0.0204\n",
            "Epoch 3, Iteration 900, loss = 0.0101\n",
            "Epoch 3, Iteration 1000, loss = 0.1359\n",
            "validation accuracy on validation set: 97.5999984741211\n",
            "Epoch 4, Iteration 0, loss = 0.0816\n",
            "Epoch 4, Iteration 100, loss = 0.0386\n",
            "Epoch 4, Iteration 200, loss = 0.1054\n",
            "Epoch 4, Iteration 300, loss = 0.1894\n",
            "Epoch 4, Iteration 400, loss = 0.0121\n",
            "Epoch 4, Iteration 500, loss = 0.0561\n",
            "Epoch 4, Iteration 600, loss = 0.0103\n",
            "Epoch 4, Iteration 700, loss = 0.1485\n",
            "Epoch 4, Iteration 800, loss = 0.2132\n",
            "Epoch 4, Iteration 900, loss = 0.0164\n",
            "Epoch 4, Iteration 1000, loss = 0.1058\n",
            "validation accuracy on validation set: 97.94999694824219\n",
            "Epoch 5, Iteration 0, loss = 0.0488\n",
            "Epoch 5, Iteration 100, loss = 0.0415\n",
            "Epoch 5, Iteration 200, loss = 0.0294\n",
            "Epoch 5, Iteration 300, loss = 0.0218\n",
            "Epoch 5, Iteration 400, loss = 0.0347\n",
            "Epoch 5, Iteration 500, loss = 0.0620\n",
            "Epoch 5, Iteration 600, loss = 0.0883\n",
            "Epoch 5, Iteration 700, loss = 0.0371\n",
            "Epoch 5, Iteration 800, loss = 0.0815\n",
            "Epoch 5, Iteration 900, loss = 0.0417\n",
            "Epoch 5, Iteration 1000, loss = 0.0571\n",
            "validation accuracy on validation set: 97.71666717529297\n",
            "Epoch 6, Iteration 0, loss = 0.0126\n",
            "Epoch 6, Iteration 100, loss = 0.0296\n",
            "Epoch 6, Iteration 200, loss = 0.0263\n",
            "Epoch 6, Iteration 300, loss = 0.0144\n",
            "Epoch 6, Iteration 400, loss = 0.2388\n",
            "Epoch 6, Iteration 500, loss = 0.0138\n",
            "Epoch 6, Iteration 600, loss = 0.0239\n",
            "Epoch 6, Iteration 700, loss = 0.0461\n",
            "Epoch 6, Iteration 800, loss = 0.0056\n",
            "Epoch 6, Iteration 900, loss = 0.0109\n",
            "Epoch 6, Iteration 1000, loss = 0.0258\n",
            "validation accuracy on validation set: 97.73332977294922\n",
            "Epoch 7, Iteration 0, loss = 0.0375\n",
            "Epoch 7, Iteration 100, loss = 0.0770\n",
            "Epoch 7, Iteration 200, loss = 0.0209\n",
            "Epoch 7, Iteration 300, loss = 0.0566\n",
            "Epoch 7, Iteration 400, loss = 0.0183\n",
            "Epoch 7, Iteration 500, loss = 0.0099\n",
            "Epoch 7, Iteration 600, loss = 0.0862\n",
            "Epoch 7, Iteration 700, loss = 0.0511\n",
            "Epoch 7, Iteration 800, loss = 0.0277\n",
            "Epoch 7, Iteration 900, loss = 0.0099\n",
            "Epoch 7, Iteration 1000, loss = 0.0629\n",
            "validation accuracy on validation set: 97.46666717529297\n",
            "Epoch 8, Iteration 0, loss = 0.0400\n",
            "Epoch 8, Iteration 100, loss = 0.0191\n",
            "Epoch 8, Iteration 200, loss = 0.0800\n",
            "Epoch 8, Iteration 300, loss = 0.0394\n",
            "Epoch 8, Iteration 400, loss = 0.0061\n",
            "Epoch 8, Iteration 500, loss = 0.1187\n",
            "Epoch 8, Iteration 600, loss = 0.0053\n",
            "Epoch 8, Iteration 700, loss = 0.0726\n",
            "Epoch 8, Iteration 800, loss = 0.1306\n",
            "Epoch 8, Iteration 900, loss = 0.0119\n",
            "Epoch 8, Iteration 1000, loss = 0.0410\n",
            "validation accuracy on validation set: 97.73332977294922\n",
            "EARLY STOP DUE TO DECREASED VALIDATION ACCURACY: USING MODEL AT EPOCH 4\n",
            "best model validation accuracy: 97.94999694824219\n"
          ]
        }
      ],
      "source": [
        "mnistNet1 = MnistNet()\n",
        "# Using Adam optimizer with suggested parameters from class, learning rate = 0.001, betas = 0.9, 0.999\n",
        "optimizer = optim.Adam(mnistNet1.parameters(), lr = 0.0005, betas=(0.9,0.999))\n",
        "\n",
        "# Train model on training data and save parameters as mnist_model.pth\n",
        "# Early stopping at around epoch 5, roughly 97.8% accuracy\n",
        "train(mnist_train_data_loader, mnist_test_data_loader, mnistNet1, optimizer, epochs= 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval 9795 / 10000 correct (97.95)\n"
          ]
        }
      ],
      "source": [
        "# Load model and test it on the testing set\n",
        "test(mnist_test_data_loader, mnistNet1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
